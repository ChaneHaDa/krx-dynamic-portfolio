"""ETL íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"""

import argparse
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import pandas as pd

from .data_loader import KRXDataLoader
from .preprocessor import KRXPreprocessor


def setup_cache_directory(cache_path: Path) -> None:
    """ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •"""
    cache_path.mkdir(parents=True, exist_ok=True)
    (cache_path / "raw").mkdir(exist_ok=True)
    (cache_path / "processed").mkdir(exist_ok=True)
    (cache_path / "features").mkdir(exist_ok=True)


def run_etl_pipeline(
    data_root: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    cache_path: str = "./data/cache",
    force_reload: bool = False
) -> None:
    """ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        data_root: KRX JSON ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
        start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD, Noneì´ë©´ ìµœê·¼ 30ì¼)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD, Noneì´ë©´ ì˜¤ëŠ˜)
        cache_path: ìºì‹œ ì €ì¥ ê²½ë¡œ
        force_reload: ê°•ì œ ë¦¬ë¡œë“œ ì—¬ë¶€
    """
    
    print("ğŸš€ KRX Dynamic Portfolio ETL Pipeline ì‹œì‘")
    print("=" * 60)
    
    # 1. ì´ˆê¸°í™”
    cache_dir = Path(cache_path)
    setup_cache_directory(cache_dir)
    
    loader = KRXDataLoader(data_root)
    preprocessor = KRXPreprocessor(min_market_cap=100e8, min_volume=1000)
    
    # 2. ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    if not end_date:
        end_date = datetime.now().strftime("%Y%m%d")
    if not start_date:
        start_dt = datetime.strptime(end_date, "%Y%m%d") - timedelta(days=30)
        start_date = start_dt.strftime("%Y%m%d")
    
    print(f"ğŸ“… ë°ì´í„° ë¡œë”© ê¸°ê°„: {start_date} ~ {end_date}")
    
    # 3. ìºì‹œ í™•ì¸
    raw_cache_file = cache_dir / "raw" / f"krx_data_{start_date}_{end_date}.parquet"
    
    if raw_cache_file.exists() and not force_reload:
        print(f"ğŸ“‚ ìºì‹œì—ì„œ ì›ì‹œ ë°ì´í„° ë¡œë“œ: {raw_cache_file}")
        raw_df = pd.read_parquet(raw_cache_file)
    else:
        # 4. ì›ì‹œ ë°ì´í„° ë¡œë”©
        print("ğŸ“¥ KRX JSON ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            raw_df = loader.load_date_range(start_date, end_date)
            print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(raw_df):,}ê±´")
            
            # ì›ì‹œ ë°ì´í„° ìºì‹œ ì €ì¥
            raw_df.to_parquet(raw_cache_file)
            print(f"ğŸ’¾ ì›ì‹œ ë°ì´í„° ìºì‹œ ì €ì¥: {raw_cache_file}")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    # 5. ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ
    print("\nğŸ“Š ì›ì‹œ ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ:")
    quality_report = preprocessor.get_data_quality_report(raw_df)
    print_quality_report(quality_report)
    
    # 6. ë°ì´í„° ì „ì²˜ë¦¬
    print("\nğŸ§¹ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    processed_cache_file = cache_dir / "processed" / f"krx_processed_{start_date}_{end_date}.parquet"
    
    if processed_cache_file.exists() and not force_reload:
        print(f"ğŸ“‚ ìºì‹œì—ì„œ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ: {processed_cache_file}")
        processed_df = pd.read_parquet(processed_cache_file)
    else:
        processed_df = preprocessor.clean_data(raw_df)
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_df):,}ê±´ (ì œê±°: {len(raw_df) - len(processed_df):,}ê±´)")
        
        # ì „ì²˜ë¦¬ ë°ì´í„° ìºì‹œ ì €ì¥
        processed_df.to_parquet(processed_cache_file)
        print(f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ìºì‹œ ì €ì¥: {processed_cache_file}")
    
    # 7. ë¶„ì„ìš© ë°ì´í„°ì…‹ ìƒì„±
    print("\nğŸ”§ ë¶„ì„ìš© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    
    # OHLCV íŒ¨ë„ ë°ì´í„°
    ohlcv_file = cache_dir / "features" / f"ohlcv_panel_{start_date}_{end_date}.parquet"
    if not ohlcv_file.exists() or force_reload:
        ohlcv_df = preprocessor.create_ohlcv_panel(processed_df)
        ohlcv_df.to_parquet(ohlcv_file)
        print(f"âœ… OHLCV íŒ¨ë„ ìƒì„±: {ohlcv_df.shape}")
    else:
        print(f"ğŸ“‚ OHLCV íŒ¨ë„ ìºì‹œ ì‚¬ìš©: {ohlcv_file}")
    
    # ìˆ˜ìµë¥  ë§¤íŠ¸ë¦­ìŠ¤
    returns_file = cache_dir / "features" / f"daily_returns_{start_date}_{end_date}.parquet"
    if not returns_file.exists() or force_reload:
        ohlcv_df = pd.read_parquet(ohlcv_file)
        returns_df = preprocessor.create_returns_matrix(ohlcv_df, period='1D')
        returns_df.to_parquet(returns_file)
        print(f"âœ… ì¼ì¼ ìˆ˜ìµë¥  ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±: {returns_df.shape}")
    else:
        print(f"ğŸ“‚ ìˆ˜ìµë¥  ë§¤íŠ¸ë¦­ìŠ¤ ìºì‹œ ì‚¬ìš©: {returns_file}")
    
    # ì‹œê°€ì´ì•¡ ê°€ì¤‘ì¹˜
    weights_file = cache_dir / "features" / f"market_cap_weights_{start_date}_{end_date}.parquet"
    if not weights_file.exists() or force_reload:
        weights_df = preprocessor.create_market_cap_weights(processed_df)
        weights_df.to_parquet(weights_file)
        print(f"âœ… ì‹œê°€ì´ì•¡ ê°€ì¤‘ì¹˜ ìƒì„±: {weights_df.shape}")
    else:
        print(f"ğŸ“‚ ì‹œê°€ì´ì•¡ ê°€ì¤‘ì¹˜ ìºì‹œ ì‚¬ìš©: {weights_file}")
    
    # 8. íˆ¬ì ìœ ë‹ˆë²„ìŠ¤ ìƒì„±
    universe_file = cache_dir / "features" / f"investment_universe_{end_date}.json"
    if not universe_file.exists() or force_reload:
        # KOSPI ìƒìœ„ 100ê°œ
        kospi_universe = preprocessor.filter_investable_universe(
            processed_df, top_n=100, market='KOSPI'
        )
        # KOSDAQ ìƒìœ„ 50ê°œ
        kosdaq_universe = preprocessor.filter_investable_universe(
            processed_df, top_n=50, market='KOSDAQ'
        )
        
        universe_data = {
            'date': end_date,
            'kospi_top100': kospi_universe,
            'kosdaq_top50': kosdaq_universe,
            'combined_top150': kospi_universe + kosdaq_universe[:50]
        }
        
        import json
        with open(universe_file, 'w', encoding='utf-8') as f:
            json.dump(universe_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… íˆ¬ì ìœ ë‹ˆë²„ìŠ¤ ìƒì„±: KOSPI {len(kospi_universe)}ê°œ, KOSDAQ {len(kosdaq_universe)}ê°œ")
    else:
        print(f"ğŸ“‚ íˆ¬ì ìœ ë‹ˆë²„ìŠ¤ ìºì‹œ ì‚¬ìš©: {universe_file}")
    
    print(f"\nğŸ‰ ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“ ìºì‹œ ìœ„ì¹˜: {cache_dir.absolute()}")
    print("=" * 60)


def print_quality_report(report: dict) -> None:
    """ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ì¶œë ¥"""
    print(f"  â€¢ ì „ì²´ ë ˆì½”ë“œ: {report['total_records']:,}ê±´")
    print(f"  â€¢ ë°ì´í„° ê¸°ê°„: {report['date_range']['start']} ~ {report['date_range']['end']} "
          f"({report['date_range']['trading_days']}ì¼)")
    print(f"  â€¢ ì¢…ëª© ìˆ˜: ì „ì²´ {report['universe_size']['total_stocks']:,}ê°œ "
          f"(KOSPI {report['universe_size']['kospi_stocks']:,}ê°œ, "
          f"KOSDAQ {report['universe_size']['kosdaq_stocks']:,}ê°œ)")
    print(f"  â€¢ ì¼í‰ê·  ì¢…ëª© ìˆ˜: {report['market_coverage']['avg_daily_stocks']:.0f}ê°œ")
    
    if report['missing_data']:
        print(f"  â€¢ ê²°ì¸¡ì¹˜: {len(report['missing_data'])}ê°œ ì»¬ëŸ¼")
    else:
        print(f"  â€¢ ê²°ì¸¡ì¹˜: ì—†ìŒ âœ…")


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="KRX Dynamic Portfolio ETL Pipeline")
    parser.add_argument(
        '--data-root', 
        required=True,
        help="KRX JSON ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ (ì˜ˆ: /home/chan/code/vscode/python3/krx-json-data)"
    )
    parser.add_argument('--start-date', help="ì‹œì‘ ë‚ ì§œ (YYYYMMDD)")
    parser.add_argument('--end-date', help="ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)")
    parser.add_argument('--cache-path', default="./data/cache", help="ìºì‹œ ì €ì¥ ê²½ë¡œ")
    parser.add_argument('--force-reload', action='store_true', help="ê°•ì œ ë¦¬ë¡œë“œ")
    
    args = parser.parse_args()
    
    run_etl_pipeline(
        data_root=args.data_root,
        start_date=args.start_date,
        end_date=args.end_date,
        cache_path=args.cache_path,
        force_reload=args.force_reload
    )


if __name__ == "__main__":
    main()