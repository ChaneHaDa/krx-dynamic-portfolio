"""
ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
=================

ëŒ€ì‹œë³´ë“œ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import pandas as pd
import numpy as np
import streamlit as st
from typing import Dict, List, Optional, Any, Tuple
import psutil
import gc
import logging
from functools import wraps
import time

logger = logging.getLogger(__name__)


def monitor_memory(func):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # ì‹¤í–‰ ì „ ë©”ëª¨ë¦¬ ì²´í¬
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        start_time = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start_time
        
        # ì‹¤í–‰ í›„ ë©”ëª¨ë¦¬ ì²´í¬
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_diff = memory_after - memory_before
        
        logger.info(f"{func.__name__}: {execution_time:.2f}s, Memory: {memory_diff:+.1f}MB")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ í¬ë©´ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        if memory_diff > 100:  # 100MB ì´ˆê³¼
            gc.collect()
            logger.info(f"Garbage collection executed for {func.__name__}")
            
        return result
    return wrapper


class DataSampler:
    """ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§ ë° ì²­í¬ ì²˜ë¦¬"""
    
    @staticmethod
    def smart_sample(df: pd.DataFrame, target_size: int = 1000, method: str = "uniform") -> pd.DataFrame:
        """
        ì§€ëŠ¥í˜• ë°ì´í„° ìƒ˜í”Œë§
        
        Parameters
        ----------
        df : pd.DataFrame
            ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        target_size : int
            ëª©í‘œ í¬ê¸°
        method : str
            ìƒ˜í”Œë§ ë°©ë²• ('uniform', 'stratified', 'recent')
        
        Returns
        -------
        pd.DataFrame
            ìƒ˜í”Œë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        if len(df) <= target_size:
            return df.copy()
        
        if method == "uniform":
            # ê· ë“± ê°„ê²© ìƒ˜í”Œë§
            step = len(df) // target_size
            return df.iloc[::step].copy()
            
        elif method == "recent":
            # ìµœê·¼ ë°ì´í„° ìš°ì„  ìƒ˜í”Œë§ (ì‹œê³„ì—´ ë°ì´í„°ìš©)
            recent_ratio = 0.7  # ìµœê·¼ 70% ë°ì´í„°ì—ì„œ ë” ë§ì´ ìƒ˜í”Œë§
            recent_size = int(target_size * recent_ratio)
            old_size = target_size - recent_size
            
            split_idx = len(df) // 2
            recent_data = df.iloc[split_idx:].sample(min(recent_size, len(df) - split_idx))
            old_data = df.iloc[:split_idx].sample(min(old_size, split_idx))
            
            return pd.concat([old_data, recent_data]).sort_index()
            
        elif method == "stratified":
            # ê³„ì¸µì  ìƒ˜í”Œë§ (ì¹´í…Œê³ ë¦¬ê°€ ìˆëŠ” ê²½ìš°)
            if hasattr(df, 'sector') or 'sector' in df.columns:
                return df.groupby('sector').apply(
                    lambda x: x.sample(min(len(x), target_size // df['sector'].nunique()))
                ).reset_index(drop=True)
            else:
                return df.sample(target_size)
        
        return df.sample(target_size)
    
    @staticmethod
    def chunk_processor(data: pd.DataFrame, chunk_size: int = 100, 
                       process_func=None) -> List[Any]:
        """
        ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬
        
        Parameters
        ----------
        data : pd.DataFrame
            ì²˜ë¦¬í•  ë°ì´í„°
        chunk_size : int
            ì²­í¬ í¬ê¸°
        process_func : callable
            ê° ì²­í¬ì— ì ìš©í•  í•¨ìˆ˜
        
        Returns
        -------
        List[Any]
            ì²˜ë¦¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if process_func is None:
            process_func = lambda x: x
            
        results = []
        total_chunks = (len(data) - 1) // chunk_size + 1
        
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i + chunk_size]
            result = process_func(chunk)
            results.append(result)
            
            # ì§„í–‰ë¥  í‘œì‹œ (Streamlitì—ì„œ)
            if hasattr(st, 'progress'):
                progress = (i // chunk_size + 1) / total_chunks
                st.progress(progress)
        
        return results


class StreamlitOptimizer:
    """Streamlit íŠ¹í™” ìµœì í™”"""
    
    @staticmethod
    def lazy_load_dataframe(data_loader_func, key: str, refresh_button: bool = True):
        """
        ì§€ì—° ë¡œë”© ë°ì´í„°í”„ë ˆì„
        
        Parameters
        ----------
        data_loader_func : callable
            ë°ì´í„° ë¡œë”© í•¨ìˆ˜
        key : str
            ì„¸ì…˜ ìƒíƒœ í‚¤
        refresh_button : bool
            ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ í‘œì‹œ ì—¬ë¶€
        
        Returns
        -------
        pd.DataFrame or None
            ë¡œë”©ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
        if refresh_button:
            col1, col2 = st.columns([3, 1])
            with col2:
                if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", key=f"refresh_{key}"):
                    if key in st.session_state:
                        del st.session_state[key]
        
        # ë°ì´í„° ë¡œë”©
        if key not in st.session_state:
            with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
                try:
                    st.session_state[key] = data_loader_func()
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
                    return None
        
        return st.session_state[key]
    
    @staticmethod
    def optimized_dataframe_display(df: pd.DataFrame, max_rows: int = 100):
        """
        ìµœì í™”ëœ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
        
        Parameters
        ----------
        df : pd.DataFrame
            í‘œì‹œí•  ë°ì´í„°í”„ë ˆì„
        max_rows : int
            ìµœëŒ€ í‘œì‹œ í–‰ ìˆ˜
        """
        if len(df) > max_rows:
            st.info(f"ğŸ“Š ì „ì²´ {len(df):,}í–‰ ì¤‘ ìƒìœ„ {max_rows}í–‰ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì˜µì…˜
            show_pagination = st.checkbox("í˜ì´ì§€ë„¤ì´ì…˜ ì‚¬ìš©")
            
            if show_pagination:
                page_size = st.selectbox("í˜ì´ì§€ í¬ê¸°", [50, 100, 200], index=1)
                total_pages = (len(df) - 1) // page_size + 1
                page = st.selectbox("í˜ì´ì§€", range(1, total_pages + 1))
                
                start_idx = (page - 1) * page_size
                end_idx = min(start_idx + page_size, len(df))
                display_df = df.iloc[start_idx:end_idx]
            else:
                display_df = df.head(max_rows)
        else:
            display_df = df
        
        st.dataframe(display_df, use_container_width=True)
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        if len(df) > 0:
            csv_data = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_data,
                file_name=f"data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )


class CacheManager:
    """Streamlit ìºì‹œ ê´€ë¦¬"""
    
    @staticmethod
    def get_cache_info() -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ì¡°íšŒ"""
        try:
            # Streamlit ìºì‹œ í†µê³„ (ê°€ëŠ¥í•œ ê²½ìš°)
            cache_stats = {
                "total_cached_functions": 0,
                "cache_hits": "N/A",
                "cache_misses": "N/A",
                "memory_usage": "N/A"
            }
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            cache_stats.update({
                "system_memory_total": f"{memory.total / 1024**3:.1f} GB",
                "system_memory_available": f"{memory.available / 1024**3:.1f} GB",
                "system_memory_percent": f"{memory.percent:.1f}%"
            })
            
            return cache_stats
        except Exception as e:
            logger.error(f"Cache info retrieval failed: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def clear_all_caches():
        """ëª¨ë“  Streamlit ìºì‹œ í´ë¦¬ì–´"""
        try:
            st.cache_data.clear()
            if hasattr(st, 'cache_resource'):
                st.cache_resource.clear()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
            gc.collect()
            
            return True, "ëª¨ë“  ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"Cache clearing failed: {e}")
            return False, f"ìºì‹œ í´ë¦¬ì–´ ì‹¤íŒ¨: {e}"
    
    @staticmethod
    def selective_cache_clear(cache_keys: List[str]):
        """ì„ íƒì  ìºì‹œ í´ë¦¬ì–´"""
        cleared_count = 0
        for key in cache_keys:
            if key in st.session_state:
                del st.session_state[key]
                cleared_count += 1
        
        if cleared_count > 0:
            gc.collect()
        
        return f"{cleared_count}ê°œ ìºì‹œ í•­ëª©ì´ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤."


class PerformanceProfiler:
    """ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë„êµ¬"""
    
    def __init__(self):
        self.start_time = None
        self.checkpoints = []
        
    def start(self):
        """í”„ë¡œíŒŒì¼ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.checkpoints = []
        
    def checkpoint(self, name: str):
        """ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€"""
        if self.start_time is None:
            self.start()
            
        current_time = time.time()
        elapsed = current_time - self.start_time
        
        if self.checkpoints:
            interval = current_time - self.checkpoints[-1]['timestamp']
        else:
            interval = elapsed
            
        self.checkpoints.append({
            'name': name,
            'elapsed_total': elapsed,
            'elapsed_interval': interval,
            'timestamp': current_time,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        })
    
    def get_report(self) -> pd.DataFrame:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë°˜í™˜"""
        if not self.checkpoints:
            return pd.DataFrame()
            
        return pd.DataFrame(self.checkpoints)
    
    def display_report(self):
        """Streamlitì—ì„œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ í‘œì‹œ"""
        report_df = self.get_report()
        
        if report_df.empty:
            st.warning("ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        st.subheader("ğŸ” ì„±ëŠ¥ í”„ë¡œíŒŒì¼ ë¦¬í¬íŠ¸")
        
        # ìš”ì•½ í†µê³„
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ì‹¤í–‰ ì‹œê°„", f"{report_df['elapsed_total'].iloc[-1]:.2f}s")
        with col2:
            st.metric("í‰ê·  ë©”ëª¨ë¦¬", f"{report_df['memory_mb'].mean():.1f} MB")
        with col3:
            st.metric("ìµœëŒ€ ë©”ëª¨ë¦¬", f"{report_df['memory_mb'].max():.1f} MB")
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        display_df = report_df[['name', 'elapsed_interval', 'elapsed_total', 'memory_mb']].copy()
        display_df.columns = ['ë‹¨ê³„', 'êµ¬ê°„ ì‹œê°„(s)', 'ëˆ„ì  ì‹œê°„(s)', 'ë©”ëª¨ë¦¬(MB)']
        display_df['êµ¬ê°„ ì‹œê°„(s)'] = display_df['êµ¬ê°„ ì‹œê°„(s)'].round(3)
        display_df['ëˆ„ì  ì‹œê°„(s)'] = display_df['ëˆ„ì  ì‹œê°„(s)'].round(3)
        display_df['ë©”ëª¨ë¦¬(MB)'] = display_df['ë©”ëª¨ë¦¬(MB)'].round(1)
        
        st.dataframe(display_df, use_container_width=True)


# ì „ì—­ í”„ë¡œíŒŒì¼ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
_performance_profiler = None

def get_profiler() -> PerformanceProfiler:
    """ì „ì—­ í”„ë¡œíŒŒì¼ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _performance_profiler
    if _performance_profiler is None:
        _performance_profiler = PerformanceProfiler()
    return _performance_profiler