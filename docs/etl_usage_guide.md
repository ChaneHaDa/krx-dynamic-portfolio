# ETL ì‚¬ìš© ê°€ì´ë“œ

KRX Dynamic Portfolio ETL íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ì‹œì‘í•˜ê¸°

### 1. ì„¤ì¹˜ ë° ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜
make install

# ê°œë°œ í™˜ê²½ ì„¤ì • (ì˜µì…˜)
make dev
```

### 2. KRX ë°ì´í„° ì¤€ë¹„

KRX JSON ë°ì´í„°ê°€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

```
/path/to/krx-json-data/
â””â”€â”€ Price/
    â””â”€â”€ STOCK/
        â”œâ”€â”€ 2023/
        â”‚   â”œâ”€â”€ 20231201.json
        â”‚   â”œâ”€â”€ 20231202.json
        â”‚   â””â”€â”€ ...
        â””â”€â”€ 2024/
            â”œâ”€â”€ 20240101.json
            â””â”€â”€ ...
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. CLIë¡œ ETL ì‹¤í–‰

ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# ê¸°ë³¸ ì‹¤í–‰ (ìµœê·¼ 30ì¼ ë°ì´í„°)
python -m krx_portfolio.etl.main --data-root /path/to/krx-json-data

# íŠ¹ì • ê¸°ê°„ ì‹¤í–‰
python -m krx_portfolio.etl.main \
    --data-root /path/to/krx-json-data \
    --start-date 20231201 \
    --end-date 20231215

# ìºì‹œ ê²½ë¡œ ì§€ì •
python -m krx_portfolio.etl.main \
    --data-root /path/to/krx-json-data \
    --cache-path ./my_cache \
    --force-reload
```

### 2. Python ì½”ë“œë¡œ ETL ì‹¤í–‰

```python
from krx_portfolio.etl.main import run_etl_pipeline

# ê¸°ë³¸ ì‹¤í–‰
run_etl_pipeline(
    data_root="/path/to/krx-json-data",
    start_date="20231201",
    end_date="20231215"
)
```

## ë‹¨ê³„ë³„ ì‚¬ìš©ë²•

### 1. ë°ì´í„° ë¡œë”©

```python
from krx_portfolio.etl.data_loader import KRXDataLoader

# ë¡œë” ì´ˆê¸°í™”
loader = KRXDataLoader("/path/to/krx-json-data")

# ë‹¨ì¼ ë‚ ì§œ ë¡œë“œ
df = loader.load_single_date("20231215")
print(f"ë¡œë“œëœ ì¢…ëª© ìˆ˜: {len(df)}")

# ë‚ ì§œ ë²”ìœ„ ë¡œë“œ
df = loader.load_date_range("20231201", "20231215")
print(f"ì „ì²´ ë ˆì½”ë“œ: {len(df)}")

# ìµœê·¼ ë°ì´í„° ë¡œë“œ
df = loader.load_latest_available(days_back=7)

# ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ í™•ì¸
dates = loader.get_available_dates(year=2023)
print(f"2023ë…„ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: {len(dates)}ì¼")
```

### 2. ë°ì´í„° ì „ì²˜ë¦¬

```python
from krx_portfolio.etl.preprocessor import KRXPreprocessor

# ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (í•„í„° ì¡°ê±´ ì„¤ì •)
preprocessor = KRXPreprocessor(
    min_market_cap=100e8,  # 100ì–µì› ì´ìƒ
    min_volume=1000        # 1000ì£¼ ì´ìƒ
)

# ë°ì´í„° ì •ì œ
cleaned_df = preprocessor.clean_data(raw_df)
print(f"ì •ì œ í›„ ë ˆì½”ë“œ: {len(cleaned_df)}")

# ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ
report = preprocessor.get_data_quality_report(cleaned_df)
print(f"ì´ {report['universe_size']['total_stocks']}ê°œ ì¢…ëª©")
print(f"KOSPI: {report['universe_size']['kospi_stocks']}ê°œ")
print(f"KOSDAQ: {report['universe_size']['kosdaq_stocks']}ê°œ")
```

### 3. ë¶„ì„ìš© ë°ì´í„° ìƒì„±

```python
# OHLCV íŒ¨ë„ ë°ì´í„° ìƒì„±
ohlcv_df = preprocessor.create_ohlcv_panel(cleaned_df)
print(f"OHLCV íŒ¨ë„ í¬ê¸°: {ohlcv_df.shape}")

# ì¼ì¼ ìˆ˜ìµë¥  ë§¤íŠ¸ë¦­ìŠ¤
returns_df = preprocessor.create_returns_matrix(ohlcv_df, period='1D')
print(f"ìˆ˜ìµë¥  ë§¤íŠ¸ë¦­ìŠ¤: {returns_df.shape}")

# ì‹œê°€ì´ì•¡ ê°€ì¤‘ì¹˜
weights_df = preprocessor.create_market_cap_weights(cleaned_df)
print(f"ê°€ì¤‘ì¹˜ ë§¤íŠ¸ë¦­ìŠ¤: {weights_df.shape}")

# íˆ¬ì ìœ ë‹ˆë²„ìŠ¤ ìƒì„±
kospi_top100 = preprocessor.filter_investable_universe(
    cleaned_df, top_n=100, market='KOSPI'
)
kosdaq_top50 = preprocessor.filter_investable_universe(
    cleaned_df, top_n=50, market='KOSDAQ'
)
print(f"KOSPI Top 100: {len(kospi_top100)}ê°œ")
print(f"KOSDAQ Top 50: {len(kosdaq_top50)}ê°œ")
```

## ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ë°ì´í„° í’ˆì§ˆ ë¶„ì„

```python
# ìƒì„¸ ë°ì´í„° í’ˆì§ˆ ë¶„ì„
def analyze_data_quality(df):
    report = preprocessor.get_data_quality_report(df)
    
    print("=== ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ===")
    print(f"ì „ì²´ ë ˆì½”ë“œ: {report['total_records']:,}ê±´")
    print(f"ë°ì´í„° ê¸°ê°„: {report['date_range']['start']} ~ {report['date_range']['end']}")
    print(f"ê±°ë˜ì¼ìˆ˜: {report['date_range']['trading_days']}ì¼")
    
    print(f"\n=== ì¢…ëª© ì •ë³´ ===")
    print(f"ì „ì²´ ì¢…ëª©: {report['universe_size']['total_stocks']:,}ê°œ")
    print(f"KOSPI: {report['universe_size']['kospi_stocks']:,}ê°œ")
    print(f"KOSDAQ: {report['universe_size']['kosdaq_stocks']:,}ê°œ")
    
    print(f"\n=== ì‹œì¥ ì»¤ë²„ë¦¬ì§€ ===")
    print(f"ì¼í‰ê·  ì¢…ëª©ìˆ˜: {report['market_coverage']['avg_daily_stocks']:.1f}ê°œ")
    print(f"ìµœì†Œ ì¢…ëª©ìˆ˜: {report['market_coverage']['min_daily_stocks']}ê°œ")
    print(f"ìµœëŒ€ ì¢…ëª©ìˆ˜: {report['market_coverage']['max_daily_stocks']}ê°œ")
    
    if report['missing_data']:
        print(f"\n=== ê²°ì¸¡ì¹˜ ì •ë³´ ===")
        for col, count in report['missing_data'].items():
            print(f"{col}: {count}ê±´")
    else:
        print(f"\n=== ê²°ì¸¡ì¹˜: ì—†ìŒ âœ… ===")

analyze_data_quality(cleaned_df)
```

### 2. ì‹œì¥ ë¶„ì„

```python
# ì‹œì¥ë³„ í†µê³„ ë¶„ì„
def market_analysis(df, date):
    summary = loader.get_market_summary(date)
    
    print(f"=== {date} ì‹œì¥ í˜„í™© ===")
    print(f"ì „ì²´ ì¢…ëª©: {summary['total_stocks']}ê°œ")
    
    print(f"\n=== KOSPI ===")
    print(f"ì¢…ëª©ìˆ˜: {summary['kospi']['count']:,}ê°œ")
    print(f"ì‹œê°€ì´ì•¡: {summary['kospi']['total_market_cap']/1e12:.1f}ì¡°ì›")
    print(f"ê±°ë˜ëŸ‰: {summary['kospi']['total_volume']/1e8:.1f}ì–µì£¼")
    
    print(f"\n=== KOSDAQ ===")
    print(f"ì¢…ëª©ìˆ˜: {summary['kosdaq']['count']:,}ê°œ")
    print(f"ì‹œê°€ì´ì•¡: {summary['kosdaq']['total_market_cap']/1e12:.1f}ì¡°ì›")
    print(f"ê±°ë˜ëŸ‰: {summary['kosdaq']['total_volume']/1e8:.1f}ì–µì£¼")

market_analysis(cleaned_df, "20231215")
```

### 3. ìˆ˜ìµë¥  ë¶„ì„

```python
import numpy as np

# ìˆ˜ìµë¥  í†µê³„ ë¶„ì„
def returns_analysis(returns_df):
    print("=== ìˆ˜ìµë¥  ë¶„ì„ ===")
    
    # ê¸°ë³¸ í†µê³„
    mean_returns = returns_df.mean()
    std_returns = returns_df.std()
    
    print(f"í‰ê·  ì¼ì¼ìˆ˜ìµë¥ : {mean_returns.mean()*100:.3f}%")
    print(f"í‰ê·  ë³€ë™ì„±: {std_returns.mean()*100:.3f}%")
    
    # ìƒìœ„/í•˜ìœ„ ì¢…ëª©
    top_performers = mean_returns.nlargest(5)
    worst_performers = mean_returns.nsmallest(5)
    
    print(f"\n=== ìƒìœ„ 5ê°œ ì¢…ëª© (í‰ê·  ì¼ì¼ìˆ˜ìµë¥ ) ===")
    for stock, ret in top_performers.items():
        print(f"{stock}: {ret*100:.3f}%")
    
    print(f"\n=== í•˜ìœ„ 5ê°œ ì¢…ëª© (í‰ê·  ì¼ì¼ìˆ˜ìµë¥ ) ===")
    for stock, ret in worst_performers.items():
        print(f"{stock}: {ret*100:.3f}%")
    
    # ë¦¬ìŠ¤í¬ ë¶„ì„
    high_vol_stocks = std_returns.nlargest(5)
    low_vol_stocks = std_returns.nsmallest(5)
    
    print(f"\n=== ê³ ë³€ë™ì„± 5ê°œ ì¢…ëª© ===")
    for stock, vol in high_vol_stocks.items():
        print(f"{stock}: {vol*100:.3f}%")

returns_analysis(returns_df)
```

## ìºì‹± í™œìš©

### 1. ìºì‹œ êµ¬ì¡° ì´í•´

```python
import os
from pathlib import Path

def explore_cache(cache_path="./data/cache"):
    cache_dir = Path(cache_path)
    
    print("=== ìºì‹œ êµ¬ì¡° ===")
    for root, dirs, files in os.walk(cache_dir):
        level = root.replace(str(cache_dir), '').count(os.sep)
        indent = ' ' * 2 * level
        print(f"{indent}{os.path.basename(root)}/")
        
        subindent = ' ' * 2 * (level + 1)
        for file in files:
            file_path = Path(root) / file
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"{subindent}{file} ({size_mb:.1f}MB)")

explore_cache()
```

### 2. ìºì‹œ ê´€ë¦¬

```python
def clear_cache(cache_path="./data/cache", keep_raw=True):
    """ìºì‹œ ì •ë¦¬ (ì›ì‹œ ë°ì´í„°ëŠ” ë³´ì¡´ ì˜µì…˜)"""
    cache_dir = Path(cache_path)
    
    dirs_to_clear = ["processed", "features"]
    if not keep_raw:
        dirs_to_clear.append("raw")
    
    for dir_name in dirs_to_clear:
        dir_path = cache_dir / dir_name
        if dir_path.exists():
            for file in dir_path.iterdir():
                file.unlink()
                print(f"ì‚­ì œë¨: {file}")

def cache_info(cache_path="./data/cache"):
    """ìºì‹œ ì •ë³´ í™•ì¸"""
    cache_dir = Path(cache_path)
    
    for subdir in ["raw", "processed", "features"]:
        subdir_path = cache_dir / subdir
        if subdir_path.exists():
            files = list(subdir_path.glob("*"))
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            print(f"{subdir}: {len(files)}ê°œ íŒŒì¼, {total_size/1024/1024:.1f}MB")

cache_info()
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬

```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
def process_large_dataset(data_root, start_date, end_date, chunk_days=7):
    """ì²­í¬ ë‹¨ìœ„ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬"""
    from datetime import datetime, timedelta
    
    start_dt = datetime.strptime(start_date, "%Y%m%d")
    end_dt = datetime.strptime(end_date, "%Y%m%d")
    
    current_dt = start_dt
    all_results = []
    
    while current_dt <= end_dt:
        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)
        
        print(f"ì²˜ë¦¬ ì¤‘: {current_dt.strftime('%Y%m%d')} ~ {chunk_end.strftime('%Y%m%d')}")
        
        # ì²­í¬ ë‹¨ìœ„ ETL ì‹¤í–‰
        run_etl_pipeline(
            data_root=data_root,
            start_date=current_dt.strftime("%Y%m%d"),
            end_date=chunk_end.strftime("%Y%m%d"),
            cache_path=f"./cache_chunk_{current_dt.strftime('%Y%m%d')}"
        )
        
        current_dt = chunk_end + timedelta(days=1)

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
def monitor_memory():
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    print(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")

monitor_memory()
```

### 2. ë³‘ë ¬ ì²˜ë¦¬ (í–¥í›„ êµ¬í˜„ ì˜ˆì •)

```python
# í–¥í›„ ë²„ì „ì—ì„œ ì§€ì›ë  ë³‘ë ¬ ì²˜ë¦¬ ì˜ˆì‹œ
def parallel_etl_pipeline(data_root, date_list, n_workers=4):
    """ë³‘ë ¬ ETL ì²˜ë¦¬ (í–¥í›„ êµ¬í˜„ ì˜ˆì •)"""
    from concurrent.futures import ProcessPoolExecutor
    
    def process_single_date(date):
        return run_etl_pipeline(
            data_root=data_root,
            start_date=date,
            end_date=date,
            cache_path=f"./cache_{date}"
        )
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        results = list(executor.map(process_single_date, date_list))
    
    return results
```

## ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜

**íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ**
```python
try:
    df = loader.load_single_date("20231215")
except FileNotFoundError:
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ í™•ì¸
    available_dates = loader.get_available_dates(year=2023)
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: {available_dates[-5:]}")  # ìµœê·¼ 5ì¼
```

**ë©”ëª¨ë¦¬ ë¶€ì¡±**
```python
# ë” ì‘ì€ ê¸°ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
try:
    df = loader.load_date_range("20230101", "20231231")
except MemoryError:
    print("ë©”ëª¨ë¦¬ ë¶€ì¡± - ë” ì‘ì€ ê¸°ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ì„¸ìš”")
    # ì›”ë³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for month in range(1, 13):
        start_date = f"2023{month:02d}01"
        end_date = f"2023{month:02d}31"
        monthly_df = loader.load_date_range(start_date, end_date)
```

**ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ**
```python
# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í›„ ì²˜ë¦¬
report = preprocessor.get_data_quality_report(raw_df)

if report['missing_data']:
    print("âš ï¸ ê²°ì¸¡ì¹˜ ë°œê²¬:")
    for col, count in report['missing_data'].items():
        print(f"  {col}: {count}ê±´")

# ìµœì†Œ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ê²½ê³ 
if report['market_coverage']['avg_daily_stocks'] < 2000:
    print("âš ï¸ ì¼í‰ê·  ì¢…ëª©ìˆ˜ê°€ ì˜ˆìƒë³´ë‹¤ ì ìŠµë‹ˆë‹¤")
```

### 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import time

def timed_etl_pipeline(*args, **kwargs):
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ì´ í¬í•¨ëœ ETL íŒŒì´í”„ë¼ì¸"""
    start_time = time.time()
    
    try:
        result = run_etl_pipeline(*args, **kwargs)
        success = True
    except Exception as e:
        print(f"ETL ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        success = False
        result = None
    
    end_time = time.time()
    elapsed = end_time - start_time
    
    print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"ğŸ¯ ì‹¤í–‰ ê²°ê³¼: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
timed_etl_pipeline(
    data_root="/path/to/data",
    start_date="20231201",
    end_date="20231215"
)
```

## Makefile ëª…ë ¹ì–´

í¸ì˜ë¥¼ ìœ„í•´ ì œê³µë˜ëŠ” Makefile ëª…ë ¹ì–´ë“¤:

```bash
# ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
make etl

# íŠ¹ì • ë‚ ì§œ ë²”ìœ„ë¡œ ETL ì‹¤í–‰ (í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”)
DATA_ROOT=/path/to/data START_DATE=20231201 END_DATE=20231215 make etl

# ìºì‹œ ì •ë¦¬ í›„ ETL ì‹¤í–‰
make clean-cache etl

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
make test

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
make lint

# ì „ì²´ CI íŒŒì´í”„ë¼ì¸ (ë¦°íŠ¸ + í…ŒìŠ¤íŠ¸)
make ci-test
```

ì´ ê°€ì´ë“œë¥¼ í†µí•´ KRX Dynamic Portfolio ETL íŒŒì´í”„ë¼ì¸ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.